{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "manufactured-thomas",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "from pandas import read_csv\n",
    "from pandas.plotting import scatter_matrix\n",
    "from matplotlib import pyplot\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer\n",
    "impNumeric = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "impCategorical = SimpleImputer(missing_values=np.nan, \n",
    "                               strategy='most_frequent')\n",
    "\n",
    "# explicitly require this experimental feature\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "# now you can import normally from sklearn.impute\n",
    "from sklearn.impute import IterativeImputer\n",
    "impIterative = IterativeImputer(missing_values=np.nan, sample_posterior=True, imputation_order='random', random_state=1)\n",
    "\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "level-cooperation",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "super-athens",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"124 variable K-8 education dataset.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "industrial-third",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import read_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "upset-williams",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=read_csv(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "needed-rating",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "professional-three",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset[[\"P4CMPEDU\",\"P5CMPEDU\",\"P6CMPEDU\",\"C2R4MPF\",\"C4R4MPF\",\"W1INCCAT\",\"W5INCCAT\",\"W8INCCAT\",\"WKMOMED\",\"WKDADED\",\"P1HMEMP\",\"P1HDEMP\",\"P7HMEMP\",\"P7HDEMP\",\"C7R4MPF\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "devoted-france",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chubby-nicholas",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cleared-motion",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop NA's for Y-variable\n",
    "cleandf=dataset.dropna(subset=[\"C7R4MPF\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "composed-atlanta",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cleandf.groupby('C7R4MPF').size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "configured-observation",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset= cleandf[cleandf[\"C7R4MPF\"]>-9]\n",
    "print(dataset.groupby('C7R4MPF').size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73183c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inappropriate-norwegian",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group some categories together\n",
    "conditions = [\n",
    "    (dataset['C7R4MPF'] >= 3.0) & (dataset['C7R4MPF'] <= 5.0),\n",
    "    (dataset['C7R4MPF'] >= 6.0) & (dataset['C7R4MPF'] <= 7.0),\n",
    "    (dataset['C7R4MPF'] >= 8.0)& (dataset['C7R4MPF'] <= 9.0)]\n",
    "choices = ['C - Low', 'B - Middle', 'A - High']\n",
    "dataset['Score'] = np.select(conditions, choices)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fuzzy-morning",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove the C7R4MPF column, now that we have the 'Score' category \n",
    "dataset=dataset.drop('C7R4MPF',1)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compact-institute",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset.groupby('Score').size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "streaming-diversity",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset2=dataset[dataset['Score']=='C - Low'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "therapeutic-tonight",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset2.groupby('Score').size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "another-aquarium",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset2=dataset2.append(dataset[dataset['Score']=='A - High'].sample(n=756))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "regulation-basic",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "least-principle",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset2.groupby('Score').size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "different-prompt",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset2=dataset2.append(dataset[dataset['Score']=='B - Middle'].sample(n=756))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "electric-norwegian",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset2.groupby('Score').size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "explicit-tunisia",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataset2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inner-maker",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use 75% of the data for training\n",
    "0.75*2268"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spectacular-reggae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating our sample of random lines\n",
    "traindf=dataset2.sample(n=1701)\n",
    "#Listing what rows our sample is using\n",
    "traindf.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "twenty-breath",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing training dataset which imputation will be performed on\n",
    "traindf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "julian-resource",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining our validation data as rows in dataset2 that are not in our training data\n",
    "validationdf=dataset2.drop(traindf.index)\n",
    "len(validationdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "impressed-keeping",
   "metadata": {},
   "outputs": [],
   "source": [
    "# taking out all NA's from validation dataset\n",
    "validationdf=validationdf.dropna()\n",
    "len(validationdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "capable-creek",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do imputation on ALL columns of TRAIN \n",
    "X = traindf[[\"P4CMPEDU\",\"P5CMPEDU\",\"P6CMPEDU\",\"C2R4MPF\",\"C4R4MPF\",\"W1INCCAT\",\"W5INCCAT\",\"W8INCCAT\",\"WKMOMED\",\"WKDADED\",\"P1HMEMP\",\"P1HDEMP\",\"P7HMEMP\",\"P7HDEMP\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mechanical-scroll",
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "simple-console",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_imputed=pd.DataFrame(impIterative.fit_transform(X),columns=[\"P4CMPEDU\",\"P5CMPEDU\",\"P6CMPEDU\",\"C2R4MPF\",\"C4R4MPF\",\"W1INCCAT\",\"W5INCCAT\",\"W8INCCAT\",\"WKMOMED\",\"WKDADED\",\"P1HMEMP\",\"P1HDEMP\",\"P7HMEMP\",\"P7HDEMP\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "breeding-helena",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_imputed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sunset-canvas",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data_imputed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "noble-paris",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a new column in data_imputed that doesn't exist yet to tack on the y-variable\n",
    "data_imputed[\"Score\"]=traindf['Score'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "portable-oakland",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_imputed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mechanical-phone",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_imputed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mediterranean-original",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_imputed.groupby('Score').size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mighty-yield",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split-out train dataset\n",
    "array = data_imputed.values\n",
    "X_train = array[:,0:14]\n",
    "Y_train= array[:,14]\n",
    "#X_train, X_validation, Y_train, Y_validation = train_test_split(X, y, test_size=0.20, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seeing-sleeping",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spot Check Algorithms\n",
    "models = []\n",
    "models.append(('LR', LogisticRegression(solver='liblinear', multi_class='ovr')))\n",
    "models.append(('LDA', LinearDiscriminantAnalysis()))\n",
    "models.append(('KNN', KNeighborsClassifier()))\n",
    "models.append(('CART', DecisionTreeClassifier()))\n",
    "models.append(('NB', GaussianNB()))\n",
    "models.append(('SVM', SVC(gamma='auto')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "broke-aluminum",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate each model in turn\n",
    "results = []\n",
    "names = []\n",
    "for name, model in models:\n",
    "\tkfold = StratifiedKFold(n_splits=7, random_state=1, shuffle=True)\n",
    "\tcv_results = cross_val_score(model, X_train, Y_train, cv=kfold, scoring='accuracy')\n",
    "\tresults.append(cv_results)\n",
    "\tnames.append(name)\n",
    "\tprint('%s: %f (%f)' % (name, cv_results.mean(), cv_results.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pursuant-county",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set validation set\n",
    "X_validation=validationdf.iloc[:,0:14]\n",
    "Y_validation=validationdf.iloc[:,14]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15001fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "validationdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "excess-premiere",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on validation dataset\n",
    " \n",
    "model = SVC(gamma='auto')\n",
    "model.fit(X_train, Y_train)\n",
    "predictions = model.predict(X_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fundamental-funds",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate predictions\n",
    "print(accuracy_score(Y_validation, predictions))\n",
    "plot_confusion_matrix(model.fit(X_train, Y_train),X_validation,Y_validation,cmap=plt.cm.Blues)\n",
    "print(classification_report(Y_validation, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "environmental-tampa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
