{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99b5689d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "from pandas import read_csv\n",
    "from pandas.plotting import scatter_matrix\n",
    "from matplotlib import pyplot\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import sys\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "# explicitly require this experimental feature\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "# now you can import normally from sklearn.impute\n",
    "from sklearn.impute import IterativeImputer\n",
    "impIterative = IterativeImputer(missing_values=np.nan, sample_posterior=True, imputation_order='random')\n",
    "\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f1525a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"0 ECLSK_98_99_K8_CHILD_v1_0.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5efe8ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_dataset=read_csv(url, low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c40334",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "original_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14853b61",
   "metadata": {},
   "source": [
    "## Determining the Best Y-Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e034e656",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "new_dataset = original_dataset.copy()\n",
    "new_dataset = new_dataset.replace(\" \", np.NaN)\n",
    "new_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad0c2762",
   "metadata": {},
   "source": [
    "### Remove the columns with the most NA values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e368aa2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_highest_empties(df):\n",
    "    half_of_df = len(df) / 2\n",
    "    column_names = []\n",
    "    \n",
    "    for column_name, _ in df.iteritems():\n",
    "        if (df[column_name].isna().sum() > half_of_df):\n",
    "            column_names.append(column_name)\n",
    "    \n",
    "    df = df.drop(columns=column_names)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "682bea59",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "new_dataset = remove_highest_empties(new_dataset)\n",
    "new_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d087ee",
   "metadata": {},
   "source": [
    "### Remove the NA values from the Y-Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea37f8b6",
   "metadata": {},
   "source": [
    "##### Using 8th grade Math Scores as the Y-variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9438613e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "clean_df = new_dataset.copy().dropna(subset=[\"C6R4MPF\"])\n",
    "clean_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0740d44",
   "metadata": {},
   "source": [
    "### Remove the variable relating to 5th Grade and Above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf20acae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_unusable_variables(df):\n",
    "    unusable_columns = []\n",
    "    \n",
    "    for column_name, _ in df.iteritems():\n",
    "        if ('5' in column_name) or ('6' in column_name) or ('7' in column_name) or ('8' in column_name):\n",
    "            if (column_name != 'C6R4MPF'):\n",
    "                unusable_columns.append(column_name)\n",
    "    \n",
    "    df = df.drop(columns=unusable_columns)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f17ddc6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "clean_df = remove_unusable_variables(clean_df)\n",
    "clean_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78073d97",
   "metadata": {},
   "source": [
    "### Determining the Correlation Between Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3029333f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def find_correlation(df):\n",
    "#     return df.corr()\n",
    "# # find the correlation between every column and the test column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0888616",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# corr_dict = find_correlation(clean_df)\n",
    "# corr_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c77968",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# corr_dict['CS_TYPE2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c719eaa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mask = (corr_dict['C6R4MPF'] > 0.3)\n",
    "# mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad7c914",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# corr_dict['C6R4MPF'][mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6d06ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# corr_dict['C6R4MPF'].loc[((corr_dict['C6R4MPF'] > 0.3) | (corr_dict['C6R4MPF'] < -0.3))] # || < -0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088309b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mask.index.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52376a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def find_covariance(df):\n",
    "#     return df.cov()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a9deec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cov_df = find_covariance(clean_df)\n",
    "# cov_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "682dfc1b",
   "metadata": {},
   "source": [
    "## Creating a new dataset with the columns we're using"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565f7ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = original_dataset[[\"P4CMPEDU\",\"P5CMPEDU\",\"P6CMPEDU\",\"C4R4MPF\",\"C5R4MPF\",\"W1INCCAT\",\"W5INCCAT\",\"W8INCCAT\",\"WKMOMED\",\"WKDADED\",\"P1HMEMP\",\"P1HDEMP\",\"P7HMEMP\",\"P7HDEMP\",\"C6R4MPF\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c9e9d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = clean_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b9a26f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3a95af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of Non-NA values in each column\n",
    "# dataset.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14995fc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Number of NA values in each column\n",
    "# dataset.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90754980",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset.groupby('C6R4MPF').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c3f7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleandf=dataset.copy().dropna(subset=[\"C6R4MPF\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b4a465c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleandf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "781b6820",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# cleandf.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8a6088",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(cleandf.groupby('C6R4MPF').size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5219b2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset= cleandf[cleandf[\"C6R4MPF\"]>\"4\"]\n",
    "# print(dataset.groupby('C6R4MPF').size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d89bb2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67f4e2c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Group some categories together\n",
    "# conditions = [\n",
    "#     (dataset['C6R4MPF'] >= \"5.0\") & (dataset['C6R4MPF'] < \"7.0\"),\n",
    "#     (dataset['C6R4MPF'] >= \"7.0\")& (dataset['C6R4MPF'] <= \"9.0\")]\n",
    "# choices = ['C - Low', 'A - High']\n",
    "# scores = np.select(conditions, choices)\n",
    "# dataset.loc[:, ('Score')] = scores\n",
    "# dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14080eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove the C6R4MPF column, now that we have the 'Score' category \n",
    "# dataset=dataset.drop('C6R4MPF',1)\n",
    "# dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f0832d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print(dataset.groupby('Score').size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed19f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset2=dataset[dataset['Score']=='A - High'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7c555b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset2=dataset2.append(dataset[dataset['Score']=='C - Low'].sample(n=4777))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b93718e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(dataset2.groupby('Score').size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22fc369",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(dataset2.groupby('Score').size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508870a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_size = math.floor(0.75*len(dataset2))\n",
    "# sample_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd32ee56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating our sample of random lines\n",
    "# traindf=dataset2.sample(n=sample_size)\n",
    "# #Listing what rows our sample is using\n",
    "# traindf.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8818c377",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing training dataset which imputation will be performed on\n",
    "# traindf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84bbab84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining our validation data as rows in dataset2 that are not in our training data\n",
    "# validationdf=dataset2.drop(traindf.index)\n",
    "# len(validationdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb01eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# taking out all NA's from validation dataset\n",
    "# validationdf=validationdf.dropna()\n",
    "# len(validationdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d545961c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do imputation on ALL columns of TRAIN \n",
    "# X = traindf[[\"P4CMPEDU\",\"P5CMPEDU\",\"P6CMPEDU\",\"C4R4MPF\",\"C5R4MPF\",\"W1INCCAT\",\"W5INCCAT\",\"W8INCCAT\",\"WKMOMED\",\"WKDADED\",\"P1HMEMP\",\"P1HDEMP\",\"P7HMEMP\",\"P7HDEMP\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8c7516",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fcf1975",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_imputed=pd.DataFrame(impIterative.fit_transform(X),columns=[\"P4CMPEDU\",\"P5CMPEDU\",\"P6CMPEDU\",\"C4R4MPF\",\"C5R4MPF\",\"W1INCCAT\",\"W5INCCAT\",\"W8INCCAT\",\"WKMOMED\",\"WKDADED\",\"P1HMEMP\",\"P1HDEMP\",\"P7HMEMP\",\"P7HDEMP\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc1daab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_imputed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d9f663a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(data_imputed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a33d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a new column in data_imputed that doesn't exist yet to tack on the y-variable\n",
    "# data_imputed[\"Score\"]=traindf['Score'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d430e83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_imputed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e80bb9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_imputed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08031111",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(data_imputed.groupby('Score').size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7063aebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split-out train dataset\n",
    "# array = traindf.values\n",
    "# X_train = array[:,0:14]\n",
    "# Y_train= array[:,14]\n",
    "# X_train, X_validation, Y_train, Y_validation = train_test_split(X, y, test_size=0.20, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2edc30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spot Check Algorithms\n",
    "# models = []\n",
    "# models.append(('LR', LogisticRegression(max_iter=10000, multi_class='multinomial')))\n",
    "# models.append(('LDA', LinearDiscriminantAnalysis()))\n",
    "# models.append(('KNN', KNeighborsClassifier(n_neighbors=5, p=2, metric='minkowski', weights='distance')))\n",
    "# models.append(('CART', DecisionTreeClassifier()))\n",
    "# models.append(('NB', GaussianNB()))\n",
    "# models.append(('SVM', SVC(gamma='scale')))\n",
    "# models.append(('RF', RandomForestClassifier(criterion='gini', n_estimators=10000, n_jobs=2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7704fb83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # evaluate each model in turn\n",
    "# results = []\n",
    "# names = []\n",
    "# for name, model in models:\n",
    "# \tkfold = StratifiedKFold(n_splits=7, random_state=1, shuffle=True)\n",
    "# \tcv_results = cross_val_score(model, X_train, Y_train, cv=kfold, scoring='accuracy')\n",
    "# \tresults.append(cv_results)\n",
    "# \tnames.append(name)\n",
    "# \tprint('%s: %f (%f)' % (name, cv_results.mean(), cv_results.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde8163b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Set validation set\n",
    "# X_validation=validationdf.iloc[:,0:14]\n",
    "# Y_validation=validationdf.iloc[:,14]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc07c926",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Make predictions on validation dataset with SVM\n",
    " \n",
    "# model = SVC(gamma='scale')\n",
    "# model.fit(X_train, Y_train)\n",
    "# predictions = model.predict(X_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b4b3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Evaluate predictions\n",
    "# print(accuracy_score(Y_validation, predictions))\n",
    "# plot_confusion_matrix(model.fit(X_train, Y_train),X_validation,Y_validation,cmap=plt.cm.Blues)\n",
    "# print(classification_report(Y_validation, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89f6b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Make predictions on validation dataset with RF\n",
    " \n",
    "# model2 = RandomForestClassifier(criterion='gini', n_estimators=10000, n_jobs=2)\n",
    "# model2.fit(X_train, Y_train)\n",
    "# predictions2 = model2.predict(X_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1359c399",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Evaluate predictions\n",
    "# print(accuracy_score(Y_validation, predictions2))\n",
    "# plot_confusion_matrix(model2.fit(X_train, Y_train),X_validation,Y_validation,cmap=plt.cm.Blues)\n",
    "# print(classification_report(Y_validation, predictions2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7884984c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature_importance = model2.feature_importances_\n",
    "\n",
    "# sorted_idx = np.argsort(feature_importance)[::-1]\n",
    "# print(\"Feature Importance:\\n\")\n",
    "# for i in range(len(X.columns)):\n",
    "#     idx = sorted_idx[i]\n",
    "#     print(f'{X.columns[idx]:20} {feature_importance[idx]:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a87946",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
